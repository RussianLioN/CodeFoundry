# ğŸ“Š Data Pipeline Archetype

> [ğŸ  Ğ“Ğ»Ğ°Ğ²Ğ½Ğ°Ñ](../../../README.md) â†’ [ğŸ¨ Archetypes](../README.md) â†’ [ğŸ“Š Data Pipeline](#)

---

## Description

Ğ¨Ğ°Ğ±Ğ»Ğ¾Ğ½ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ ETL/ELT Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½Ğ¾Ğ², Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…Ñ… Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ½Ñ‹Ñ… Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹ Ğ¸ ML Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½Ğ¾Ğ².

---

## ğŸ¯ Ğ¥Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸

### Tech Stack

| ĞšĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚ | Ğ¢ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ |
|-----------|------------|
| **Orchestration** | Apache Airflow / Prefect |
| **Transformation** | dbt / SQL / Python |
| **Warehouse** | Snowflake / BigQuery / PostgreSQL |
| **Data Lake** | AWS S3 / GCS / Azure Blob |
| **Streaming** | Apache Kafka / AWS Kinesis |
| **Processing** | Python Pandas / Polars / Spark |
| **Monitoring** | Airflow Monitoring / Grafana |

### Features Out-of-the-Box

âœ… **Orchestrator Ready** â€” Airflow DAGs Ñ best practices
âœ… **dbt Integration** â€” Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² SQL
âœ… **Data Quality** â€” Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
âœ… **Lineage Tracking** â€” Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ğ¸ÑÑ…Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
âœ… **Monitoring** â€” alerting Ğ½Ğ° failure
âœ… **Testing** â€” unit Ñ‚ĞµÑÑ‚Ñ‹ Ğ´Ğ»Ñ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½Ğ¾Ğ²
âœ… **CI/CD** â€” Ğ´ĞµĞ¿Ğ»Ğ¾Ğ¹ DAG Ñ‡ĞµÑ€ĞµĞ· GitHub Actions
âœ… **Documentation** â€” auto-generated docs

---

## ğŸš€ Quick Start

### 1. Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ°

**Ğ§ĞµÑ€ĞµĞ· CodeFoundry (Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒĞµÑ‚ÑÑ):**
```bash
cd CodeFoundry
make new ARCHETYPE=data-pipeline NAME=my-pipeline
cd my-pipeline
```

**Ğ’Ñ€ÑƒÑ‡Ğ½ÑƒÑ:**
```bash
cp -r /path/to/CodeFoundry/templates/archetypes/data-pipeline ~/projects/my-pipeline
cd ~/projects/my-pipeline
git init
```

### 2. ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ

```bash
cp .env.example .env
nano .env
```

### 3. Ğ—Ğ°Ğ¿ÑƒÑĞº Airflow

```bash
# Docker Compose
make dev

# Ğ”Ğ¾ÑÑ‚ÑƒĞ¿ Ğº Airflow Web UI
open http://localhost:8080
```

---

## ğŸ“‚ Ğ¡Ñ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° ĞŸÑ€Ğ¾ĞµĞºÑ‚Ğ°

```
data-pipeline/
â”œâ”€â”€ ğŸ“‹ docs/
â”‚   â”œâ”€â”€ ARCHITECTURE.md
â”‚   â”œâ”€â”€ DAG_GUIDE.md
â”‚   â””â”€â”€ DBT_GUIDE.md
â”‚
â”œâ”€â”€ ğŸ³ docker/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ docker-compose.yml
â”‚
â”œâ”€â”€ ğŸ“Š dags/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ raw_ingestion/
â”‚   â”‚   â”œâ”€â”€ ingest_postgres.py
â”‚   â”‚   â””â”€â”€ ingest_api.py
â”‚   â”œâ”€â”€ transformations/
â”‚   â”‚   â”œâ”€â”€ dbt_run.py
â”‚   â”‚   â””â”€â”€ sql_transformations.py
â”‚   â””â”€â”€ monitoring/
â”‚       â”œâ”€â”€ data_quality.py
â”‚       â””â”€â”€ lineage_update.py
â”‚
â”œâ”€â”€ ğŸ“¦ dbt/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ staging/
â”‚   â”‚   â”œâ”€â”€ intermediate/
â”‚   â”‚   â””â”€â”€ marts/
â”‚   â”œâ”€â”€ seeds/
â”‚   â”œâ”€â”€ macros/
â”‚   â””â”€â”€ dbt_project.yml
â”‚
â”œâ”€â”€ ğŸ“ sql/
â”‚   â”œâ”€â”€ stored_procedures/
â”‚   â””â”€â”€ migrations/
â”‚
â”œâ”€â”€ ğŸ§ª tests/
â”‚   â”œâ”€â”€ unit/
â”‚   â”œâ”€â”€ integration/
â”‚   â””â”€â”€ data_quality/
â”‚
â”œâ”€â”€ ğŸ¤– openclaw/
â”‚   â””â”€â”€ workspace/
â”‚       â”œâ”€â”€ AGENTS.md
â”‚       â””â”€â”€ skills/
â”‚
â”œâ”€â”€ ğŸ“ src/
â”‚   â”œâ”€â”€ operators/
â”‚   â”œâ”€â”€ hooks/
â”‚   â”œâ”€â”€ sensors/
â”‚   â””â”€â”€ utils/
â”‚
â””â”€â”€ ğŸ”§ scripts/
    â”œâ”€â”€ setup-airflow.sh
    â””â”€â”€ deploy-dags.sh
```

---

## ğŸ¤– OpenClaw Integration

### Multi-Agent Configuration

```
Main Agent (ĞšĞ¾Ğ¾Ñ€Ğ´Ğ¸Ğ½Ğ°Ñ‚Ğ¾Ñ€)
    â”œâ”€â”€ Dev Agent (ĞšĞ¾Ğ´ DAG)
    â”œâ”€â”€ DataEngineer (ETL Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸)
    â”œâ”€â”€ MLEngine (ML Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸)
    â””â”€â”€ Review Agent (Ğ ĞµĞ²ÑŒÑ)
```

---

## ğŸ“Š DAG Examples

### Basic ETL DAG

```python
# dags/etl/etl_pipeline.py
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
from app.operators.extract import ExtractOperator
from app.operators.load import LoadOperator

with DAG(
    'etl_pipeline',
    default_args={
        'owner': 'data-engineering',
        'start_date': datetime(2024, 1, 1),
        'retries': 3,
        'retry_delay': timedelta(minutes=5),
    },
    schedule_interval='0 2 * * *',  # Daily at 2 AM
    catchup=False,
    tags=['etl', 'production'],
) as dag:

    extract = ExtractOperator(
        task_id='extract_from_source',
        source='postgres',
        table='users'
    )

    transform = PythonOperator(
        task_id='transform_data',
        python_callable=transform_users
    )

    load = LoadOperator(
        task_id='load_to_warehouse',
        destination='snowflake',
        table='dim_users'
    )

    extract >> transform >> load
```

### dbt DAG

```python
# dags/transformations/dbt_run.py
from airflow import DAG
from airflow.operators.bash import BashOperator
from datetime import datetime

with DAG(
    'dbt_daily_run',
    default_args={
        'owner': 'data-engineering',
        'start_date': datetime(2024, 1, 1),
    },
    schedule_interval='0 3 * * *',
    catchup=False,
) as dag:

    dbt_run = BashOperator(
        task_id='dbt_run_models',
        bash_command='cd /opt/dbt && dbt run --profiles-dir profiles',
    )

    dbt_test = BashOperator(
        task_id='dbt_test',
        bash_command='cd /opt/dbt && dbt test --profiles-dir profiles',
    )

    dbt_run >> dbt_test
```

---

## ğŸ“¦ dbt Models

### dbt Project Structure

```sql
-- models/staging/stg_users.sql
with source as (
    select * from {{ source('raw', 'users') }}
),

renamed as (
    select
        id as user_id,
        email as user_email,
        created_at
    from source
)

select * from renamed
```

```sql
-- models/marts/dim_user_metrics.sql
with user_orders as (
    select
        user_id,
        count(*) as total_orders,
        sum(amount) as lifetime_value
    from {{ ref('fct_orders') }}
    group by user_id
),

user_attributes as (
    select * from {{ ref('stg_users') }}
)

select
    u.*,
    coalesce(o.total_orders, 0) as total_orders,
    coalesce(o.lifetime_value, 0) as lifetime_value
from user_attributes u
left join user_orders o using (user_id)
```

---

## ğŸ§ª Data Quality Checks

```python
# tests/data_quality/test_data_quality.py
import pytest
from app.utils.data_quality import check_nulls, check_duplicates

def test_users_no_nulls():
    """Check that user_id has no nulls"""
    result = check_nulls('dim_users', 'user_id')
    assert result.null_count == 0

def test_users_unique():
    """Check that user_id is unique"""
    result = check_duplicates('dim_users', 'user_id')
    assert result.duplicate_count == 0

def test_data_freshness():
    """Check that data is fresh"""
    result = check_freshness('dim_users', max_age_hours=24)
    assert result.is_fresh
```

---

## ğŸ”§ Makefile Commands

```bash
make help           # Show all commands
make init           # Initialize project
make dev            # Start Airflow
make dbt-deps       # Install dbt dependencies
make dbt-run        # Run dbt models
make dbt-test       # Test dbt models
make dbt-docs       # Generate dbt docs
make test           # Run tests
make deploy-airflow # Deploy DAGs
```

---

## ğŸ“š Additional Resources

### CodeFoundry
- [ğŸ  Ğ“Ğ»Ğ°Ğ²Ğ½Ğ°Ñ](../../../README.md)
- [ğŸš€ Quick Start](../../../QUICKSTART.md)
- [ğŸ“‹ Ğ’ÑĞµ ĞÑ€Ñ…ĞµÑ‚Ğ¸Ğ¿Ñ‹](../README.md)
- [ğŸ”„ GitOps 2.0](../README.md)

### OpenClaw Integration
- [ğŸ¦ OpenClaw README](../../../openclaw/README.md)
- [ğŸ¤– Agents](../../../openclaw/workspace/AGENTS.md)
- [ğŸ¨ Skills Index](../../../openclaw/workspace/SKILLS-INDEX.md)

### Data Engineering Documentation
- [ğŸ“– Airflow Guide](https://airflow.apache.org/docs/)
- [ğŸ“¦ dbt Docs](https://docs.getdbt.com/)
- [ğŸ“– PostgreSQL Docs](https://www.postgresql.org/docs/)

### Pipeline Resources
- [ğŸ“– Data Engineering Guide](https://www.dataengineeringwiki.com/)
- [ğŸ“Š Best Practices](https://www.tressotechnical.com/)

---

## ğŸ”„ Ğ˜ÑÑ‚Ğ¾Ñ€Ğ¸Ñ Ğ˜Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹

| Ğ’ĞµÑ€ÑĞ¸Ñ | Ğ”Ğ°Ñ‚Ğ° | Ğ˜Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ |
|--------|------|-----------|
| 1.2.0 | 2025-01-31 | GitOps 2.0 Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½, Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ñ‹ ÑĞ»Ğ¾Ğ¼Ğ°Ğ½Ğ½Ñ‹Ğµ ÑÑÑ‹Ğ»ĞºĞ¸ |
| 1.1.0 | 2025-01-31 | CodeFoundry branding, Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»Ñ‘Ğ½Ğ½Ñ‹Ğµ breadcrumbs |
| 1.0.0 | 2025-11-05 | ĞŸĞµÑ€Ğ²Ğ°Ñ Ğ²ĞµÑ€ÑĞ¸Ñ archetype |

---

> [ğŸ  Ğ“Ğ»Ğ°Ğ²Ğ½Ğ°Ñ](../../../README.md) â†’ [ğŸ¨ Archetypes](../README.md) â†’ [ğŸ“Š Data Pipeline](#)
