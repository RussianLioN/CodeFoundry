# =============================================================================
# OpenClaw Orchestrator Stack - Environment Variables (ainetic.tech)
# =============================================================================
#
# Orchestrator Architecture v2.0 - Production/Testing Configuration
#
# Copy this file to .env.orchestrator and configure your values
# NOTE: .env.orchestrator is gitignored for security
# =============================================================================

# ============================================================
# Project Configuration
# ============================================================

# Docker Compose project name
COMPOSE_PROJECT_NAME=openclaw-orchestrator

# Docker network name
DOCKER_NETWORK=openclaw-orchestrator-net

# Project directory (absolute path on ainetic.tech)
PROJECT_DIR=/root/projects/CodeFoundry

# Project name for context
PROJECT_NAME=system-prompts

# ============================================================
# OpenClaw Gateway v2.0 Configuration
# ============================================================

# Gateway WebSocket port
GATEWAY_PORT=18789

# Gateway health check port
GATEWAY_HEALTH_PORT=18790

# Environment
NODE_ENV=production

# ============================================================
# Ollama Cloud API Configuration (CURRENT - REQUIRED)
# ============================================================

# Ollama Cloud API base URL
OLLAMA_BASE_URL=https://ollama.com

# Model to use on Ollama Cloud
# gemini-3-flash-preview:cloud - 1M context, FREE or $0.5/1M tokens
OLLAMA_MODEL=gemini-3-flash-preview:cloud

# Ollama Cloud API Key (REQUIRED)
# 1. Visit https://ollama.com/settings/keys
# 2. Sign in or create account (FREE tier available)
# 3. Generate API key
# 4. Paste below
OLLAMA_API_KEY=sk-xxxxx-xxxxx-xxxxx-xxxxx

# Ollama request timeout (milliseconds)
OLLAMA_TIMEOUT=60000

# ============================================================
# GLM-4.7-Flash Configuration (FUTURE - ORCH-011)
# ============================================================
#
# GLM-4.7-Flash is a FREE, coding-optimized model from Zhipu AI (智谱AI)
# Official Claude Code integration available
#
# Benefits:
#   - FREE tier (no API costs)
#   - Optimized for coding tasks
#   - 200K context length
#   - 128K output tokens
#   - Official Claude Code support
#
# Documentation:
#   - https://docs.z.ai/guides/llm/glm-4.7
#   - https://docs.bigmodel.cn/cn/guide/models/free/glm-4.7-flash
#
# To enable GLM-4.7-Flash:
#   1. Sign up at https://open.bigmodel.cn/
#   2. Generate API key from console
#   3. Uncomment and set ZHIPUAI_API_KEY below
#   4. Update Gateway to use GLM-4.7-Flash as model provider
#
# ZHIPUAI_API_KEY=your_key_here

# ============================================================
# CLI Bridge Configuration
# ============================================================

# Path to CLI wrapper script (inside container)
CLI_WRAPPER_PATH=/opt/claude-bridge/claude-wrapper.sh

# Claude Code container name
CLAUDE_CODE_CONTAINER=claude-code-runner

# Workspace directory (inside container)
WORKSPACE_DIR=/workspace

# ============================================================
# Command Protocol v1.0 Configuration
# ============================================================

# Command protocol version
COMMAND_PROTOCOL_VERSION=1.0

# Command execution timeout (milliseconds)
COMMAND_TIMEOUT=120000

# ============================================================
# Session Management
# ============================================================

# Session timeout (milliseconds)
# Default: 1 hour (3600000 ms)
SESSION_TIMEOUT=3600000

# Maximum concurrent sessions
MAX_SESSIONS=100

# ============================================================
# Logging Configuration
# ============================================================

# Log level: debug, info, warn, error
LOG_LEVEL=info

# Log format: json, text
LOG_FORMAT=json

# ============================================================
# Telegram Bot Integration (Optional)
# ============================================================

# Get from @BotFather on Telegram
TELEGRAM_BOT_TOKEN=your_bot_token_here

# Comma-separated list of authorized Telegram user IDs
# Get your ID from @userinfobot or @GetMyIdBot
AUTHORIZED_USER_IDS=123456789,987654321

# Bot username (optional)
BOT_USERNAME=

# Gateway reconnect settings
GATEWAY_RECONNECT_DELAY=5000
GATEWAY_MAX_RECONNECT_ATTEMPTS=10

# Session cleanup interval (milliseconds)
SESSION_CLEANUP_INTERVAL=3600000

# ============================================================
# Resource Limits (ainetic.tech)
# ============================================================

# Gateway
GATEWAY_CPU_LIMIT=2
GATEWAY_MEMORY_LIMIT=2G
GATEWAY_CPU_RESERVATION=0.5
GATEWAY_MEMORY_RESERVATION=512M

# Claude Code Runner
RUNNER_CPU_LIMIT=2
RUNNER_MEMORY_LIMIT=4G
RUNNER_CPU_RESERVATION=0.5
RUNNER_MEMORY_RESERVATION=1G

# Telegram Bot
BOT_CPU_LIMIT=1
BOT_MEMORY_LIMIT=512M
BOT_CPU_RESERVATION=0.25
BOT_MEMORY_RESERVATION=128M

# ============================================================
# Notes
# ============================================================
#
# Deployment Workflow:
#   1. Configure OLLAMA_API_KEY (get from https://ollama.com/settings/keys)
#   2. Configure TELEGRAM_BOT_TOKEN (get from @BotFather)
#   3. Configure AUTHORIZED_USER_IDS (get from @userinfobot)
#   4. docker-compose -f docker-compose.orchestrator.yml up -d
#   5. docker-compose -f docker-compose.orchestrator.yml logs -f gateway
#
# Testing:
#   # Test health check
#   curl http://localhost:18790/health
#
#   # Run unit tests
#   docker-compose -f docker-compose.orchestrator.yml exec gateway npm test
#
#   # Test CLI Bridge
#   docker-compose -f docker-compose.orchestrator.yml exec claude-code-runner \
#     bash /opt/claude-bridge/test-commands.sh
#
#   # Test via Telegram
#   Send /start, /status, /new commands to your bot
#
# Monitoring:
#   docker-compose -f docker-compose.orchestrator.yml ps
#   docker-compose -f docker-compose.orchestrator.yml logs -f gateway
#   docker-compose -f docker-compose.orchestrator.yml logs -f telegram-bot
#
# =============================================================================
